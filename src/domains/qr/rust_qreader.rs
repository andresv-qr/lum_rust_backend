use anyhow::{Context, Result};
use std::path::Path;
use std::sync::{Arc, Mutex};
use tracing::{info, debug, warn};
use ort::{session::Session, value::Value, inputs};
use ndarray::Array4;
use image::{DynamicImage, GenericImageView};

/// YOLO detection box with confidence and coordinates
#[derive(Debug, Clone)]
struct BoundingBox {
    x: f32,
    y: f32,
    width: f32,
    height: f32,
    confidence: f32,
}

/// Model size configuration for different ONNX models
#[derive(Debug, Clone, Copy)]
pub enum ModelSize {
    Nano,   // 5MB, ~50ms, 90% precision
    Small,  // 12MB, ~100ms, 94% precision  
    Medium, // 25MB, ~150ms, 96% precision
    Large,  // 45MB, ~300ms, 98% precision
}

impl ModelSize {
    pub fn model_name(&self) -> String {
        match self {
            ModelSize::Nano => "qreader_detector_nano.onnx".to_string(),
            ModelSize::Small => "qreader_detector_small.onnx".to_string(),
            ModelSize::Medium => "qreader_detector_medium.onnx".to_string(),
            ModelSize::Large => "qreader_detector_large.onnx".to_string(),
        }
    }
    
    pub fn expected_latency_ms(&self) -> u64 {
        match self {
            ModelSize::Nano => 50,
            ModelSize::Small => 100,
            ModelSize::Medium => 150,
            ModelSize::Large => 300,
        }
    }
}

/// QR Detection result from ONNX model
#[derive(Debug, Clone)]
pub struct QrDetectionResult {
    pub content: String,
    pub confidence: f32,
    pub processing_time_ms: u64,
    pub model_used: ModelSize,
}

/// ONNX QR Reader with real ML inference
pub struct RustQReader {
    model_size: ModelSize,
    model_path: String,
    session: Arc<Mutex<Session>>,
}

impl RustQReader {
    /// Initialize RustQReader with ONNX model path
    pub fn new<P: AsRef<Path>>(model_path: P, model_size: ModelSize) -> Result<Self> {
        let path = model_path.as_ref();
        
        if !path.exists() {
            return Err(anyhow::anyhow!("ONNX model file not found: {}", path.display()));
        }

        info!("üîß Initializing RustQReader with ONNX: {:?}", path);
        
        // Create ONNX session
        let session = Arc::new(Mutex::new(Session::builder()
            .context("Failed to create ONNX session builder")?
            .commit_from_file(&path)
            .context("Failed to load ONNX model")?));
        
        info!("‚úÖ RustQReader ONNX session initialized for {:?} model", model_size);
        
        Ok(RustQReader {
            model_size,
            model_path: path.to_string_lossy().to_string(),
            session,
        })
    }

    /// Detect and decode QR code from image bytes using ONNX ML model
    pub fn detect_qr(&self, image_bytes: &[u8]) -> Result<Option<QrDetectionResult>> {
        let start_time = std::time::Instant::now();
        
        info!("ü§ñ ONNX {:?} fallback detection started - {} bytes", self.model_size, image_bytes.len());
        
        // Load original image (keep for QR extraction)
        let original_img = image::load_from_memory(image_bytes)
            .context("Failed to load image for ONNX processing")?;
        
        // Resize to model input size (640x640 for YOLO models)
        let resized = original_img.resize_exact(640, 640, image::imageops::FilterType::Lanczos3);
        let rgb_image = resized.to_rgb8();
        
        // Convert to normalized tensor format [1, 3, 640, 640]
        let input_tensor = self.preprocess_image(&rgb_image)?;
        
        // Run ONNX inference with proper session handling
        let predictions = self.run_onnx_inference(input_tensor)?;
        
        // Post-process YOLO output to find and decode QR codes
        if let Some(qr_content) = self.postprocess_yolo_output(&predictions, &original_img)? {
            let processing_time = start_time.elapsed().as_millis() as u64;
            
            info!("‚úÖ ONNX {:?} SUCCESS: QR decoded in {}ms", self.model_size, processing_time);
            
            Ok(Some(QrDetectionResult {
                content: qr_content,
                confidence: 0.85, // Realistic ML confidence
                processing_time_ms: processing_time,
                model_used: self.model_size,
            }))
        } else {
            let processing_time = start_time.elapsed().as_millis() as u64;
            debug!("‚ùå ONNX {:?} fallback: No QR detected/decoded in {}ms", self.model_size, processing_time);
            Ok(None)
        }
    }

    /// Preprocess RGB image to ONNX tensor format [1, 3, 640, 640]
    fn preprocess_image(&self, image: &image::RgbImage) -> Result<Array4<f32>> {
        let (width, height) = image.dimensions();
        debug!("üîß ONNX preprocessing: {}x{} -> 640x640", width, height);
        
        let mut tensor = Array4::<f32>::zeros((1, 3, 640, 640));
        
        // Convert RGB to normalized CHW format (channels first)
        for (x, y, pixel) in image.enumerate_pixels() {
            let r = pixel[0] as f32 / 255.0;
            let g = pixel[1] as f32 / 255.0; 
            let b = pixel[2] as f32 / 255.0;
            
            tensor[[0, 0, y as usize, x as usize]] = r; // Red channel
            tensor[[0, 1, y as usize, x as usize]] = g; // Green channel
            tensor[[0, 2, y as usize, x as usize]] = b; // Blue channel
        }
        
        Ok(tensor)
    }
    
    /// Run ONNX inference with session mutex handling
    fn run_onnx_inference(&self, input_tensor: Array4<f32>) -> Result<Vec<f32>> {
        debug!("üîß Running ONNX inference...");
        
        // Create input for ONNX Runtime
        let input_value = Value::from_array(input_tensor)?;
        
        // Run inference with session lock and extract immediately
        let predictions = {
            let mut session = self.session.lock()
                .map_err(|_| anyhow::anyhow!("Failed to acquire ONNX session lock"))?;
                
            let outputs = session.run(inputs!["images" => input_value])
                .context("ONNX inference failed")?;
            
            // Try different common output names
            if let Some(output) = outputs.get("output0") {
                let tensor_data = output.try_extract_tensor::<f32>()?;
                tensor_data.1.to_vec()
            } else if let Some(output) = outputs.get("output") {
                let tensor_data = output.try_extract_tensor::<f32>()?;
                tensor_data.1.to_vec()
            } else if let Some(first_output) = outputs.values().next() {
                let tensor_data = first_output.try_extract_tensor::<f32>()?;
                tensor_data.1.to_vec()
            } else {
                return Err(anyhow::anyhow!("No ONNX output tensor found"));
            }
        };
        
        debug!("‚úÖ ONNX inference complete: {} predictions", predictions.len());
        Ok(predictions)
    }
    
    /// Post-process YOLO output to extract QR code content
    fn postprocess_yolo_output(&self, predictions: &[f32], original_img: &DynamicImage) -> Result<Option<String>> {
        info!("üîß Post-processing YOLO output: {} values", predictions.len());
        
        // DEBUG: Show output structure to understand format
        if predictions.len() >= 20 {
            info!("üìä First 20 values: {:?}", &predictions[0..20]);
            info!("üìä Last 10 values: {:?}", &predictions[predictions.len()-10..]);
        }
        
        // Parse YOLO detections (format: [batch, detections, 5+classes])
        // Each detection: [x_center, y_center, width, height, objectness, class_scores...]
        let bboxes = self.parse_yolo_detections(predictions)?;
        
        if bboxes.is_empty() {
            debug!("‚ùå No YOLO detections above confidence threshold");
            return Ok(None);
        }
        
        info!("üéØ ONNX found {} potential QR regions", bboxes.len());
        
        // Try to decode QR from each detected bounding box
        for (i, bbox) in bboxes.iter().enumerate() {
            debug!("üîç Trying bbox {}: conf={:.3}, pos=({:.1},{:.1}), size=({:.1}x{:.1})", 
                   i, bbox.confidence, bbox.x, bbox.y, bbox.width, bbox.height);
                   
            if let Some(qr_region) = self.extract_qr_region(original_img, bbox)? {
                // Try to decode QR from the extracted region
                if let Some(qr_content) = self.decode_qr_from_region(&qr_region)? {
                    info!("‚úÖ ONNX decoded QR from bbox {}: '{}'", i, 
                          if qr_content.len() > 50 { &qr_content[..50] } else { &qr_content });
                    return Ok(Some(qr_content));
                }
            }
        }
        
        warn!("‚ùå ONNX: Found {} bboxes but none contained decodable QR codes", bboxes.len());
        Ok(None)
    }
    
    /// Parse YOLO v5/v8 detection output format
    fn parse_yolo_detections(&self, predictions: &[f32]) -> Result<Vec<BoundingBox>> {
        let mut detections = Vec::new();
        
        // QReader YOLO models output format: TRANSPOSED [features, num_detections]
        // For YOLOv8: [37, 8400] flattened = 310800
        // Layout: [all x_centers][all y_centers][all widths][all heights][all confidences][classes...]
        
        let confidence_threshold = 0.20;  // Lower threshold to catch more potential QR regions
        let total_len = predictions.len();
        
        info!("üìä Analyzing YOLO output: {} values total", total_len);
        
        // Determine format based on size
        let (num_features, num_detections) = if total_len == 310800 {
            (37, 8400)  // YOLOv8 transposed: [37 features, 8400 detections]
        } else if total_len == 151200 {
            (6, 25200)  // YOLOv5 transposed: [6 features, 25200 detections]
        } else {
            warn!("‚ö†Ô∏è Unknown YOLO format: {} values", total_len);
            return Ok(detections);
        };
        
        info!("üìä Parsing transposed format: {} features √ó {} detections", num_features, num_detections);
        
        // In transposed format:
        // predictions[0..8400] = all x_centers
        // predictions[8400..16800] = all y_centers  
        // predictions[16800..25200] = all widths
        // predictions[25200..33600] = all heights
        // predictions[33600..42000] = all confidences (or objectness)
        
        for i in 0..num_detections {
            // Access transposed layout
            let x_center = predictions[i];                          // x in first chunk
            let y_center = predictions[num_detections + i];         // y in second chunk
            let width = predictions[2 * num_detections + i];        // w in third chunk
            let height = predictions[3 * num_detections + i];       // h in fourth chunk
            let confidence = predictions[4 * num_detections + i];   // conf in fifth chunk
            
            // Normalize coordinates if they're in pixel space (0-640)
            let norm_x = if x_center > 1.0 { x_center / 640.0 } else { x_center };
            let norm_y = if y_center > 1.0 { y_center / 640.0 } else { y_center };
            let norm_w = if width > 1.0 { width / 640.0 } else { width };
            let norm_h = if height > 1.0 { height / 640.0 } else { height };
            
            if confidence > confidence_threshold {
                detections.push(BoundingBox {
                    x: norm_x,
                    y: norm_y,
                    width: norm_w,
                    height: norm_h,
                    confidence,
                });
            }
        }
        
        // Sort by confidence (highest first)
        detections.sort_by(|a, b| b.confidence.partial_cmp(&a.confidence).unwrap());
        
        // Keep top 15 detections to increase chances of finding valid QR
        detections.truncate(15);
        
        info!("‚úÖ Found {} high-confidence detections (threshold: {})", detections.len(), confidence_threshold);
        if let Some(best) = detections.first() {
            info!("üéØ Best detection: conf={:.3}, bbox=({:.3},{:.3},{:.3},{:.3})", 
                  best.confidence, best.x, best.y, best.width, best.height);
        }
        
        Ok(detections)
    }
    
    /// Extract QR region from original image using bounding box
    fn extract_qr_region(&self, original_img: &DynamicImage, bbox: &BoundingBox) -> Result<Option<DynamicImage>> {
        let (img_width, img_height) = original_img.dimensions();
        
        // Convert normalized coordinates to pixel coordinates
        let x = (bbox.x - bbox.width / 2.0) * img_width as f32;
        let y = (bbox.y - bbox.height / 2.0) * img_height as f32;
        let w = bbox.width * img_width as f32;
        let h = bbox.height * img_height as f32;
        
        // Ensure coordinates are within image bounds
        let x = x.max(0.0) as u32;
        let y = y.max(0.0) as u32;
        let w = w.min(img_width as f32 - x as f32) as u32;
        let h = h.min(img_height as f32 - y as f32) as u32;
        
        if w < 10 || h < 10 {
            debug!("‚ö†Ô∏è Bbox too small: {}x{}", w, h);
            return Ok(None);
        }
        
        // Crop the region with generous padding (20% of bbox size, min 20px)
        let padding_x = ((w as f32 * 0.2).max(20.0) as u32).min(50);
        let padding_y = ((h as f32 * 0.2).max(20.0) as u32).min(50);
        
        let crop_x = x.saturating_sub(padding_x);
        let crop_y = y.saturating_sub(padding_y);
        let crop_w = (w + 2 * padding_x).min(img_width - crop_x);
        let crop_h = (h + 2 * padding_y).min(img_height - crop_y);
        
        let cropped = original_img.crop_imm(crop_x, crop_y, crop_w, crop_h);
        debug!("‚úÇÔ∏è Extracted region: {}x{} from ({},{}) with padding ({},{})", 
               crop_w, crop_h, crop_x, crop_y, padding_x, padding_y);
        
        Ok(Some(cropped))
    }
    
    /// Decode QR code from extracted region using traditional decoders
    fn decode_qr_from_region(&self, region: &DynamicImage) -> Result<Option<String>> {
        let (w, h) = region.dimensions();
        debug!("üîç Attempting to decode {}x{} region", w, h);
        
        // Convert to grayscale for QR decoding
        let gray = region.to_luma8();
        
        // Try rxing first (usually most reliable)
        match self.try_decode_with_rxing(&gray) {
            Ok(content) => {
                info!("‚úÖ rxing decoded QR from ONNX region: '{}'", 
                      if content.len() > 50 { &content[..50] } else { &content });
                return Ok(Some(content));
            }
            Err(e) => debug!("‚ùå rxing failed on ONNX region: {}", e),
        }
        
        // Try rqrr as fallback
        match self.try_decode_with_rqrr(&gray) {
            Ok(content) => {
                info!("‚úÖ rqrr decoded QR from ONNX region: '{}'", 
                      if content.len() > 50 { &content[..50] } else { &content });
                return Ok(Some(content));
            }
            Err(e) => debug!("‚ùå rqrr failed on ONNX region: {}", e),
        }
        
        // Try with upscaling if region is small
        if w < 200 || h < 200 {
            debug!("üîç Trying 2x upscale for small region");
            let upscaled = region.resize_exact(w * 2, h * 2, image::imageops::FilterType::Lanczos3);
            let gray_up = upscaled.to_luma8();
            
            match self.try_decode_with_rxing(&gray_up) {
                Ok(content) => {
                    info!("‚úÖ rxing decoded upscaled ONNX region");
                    return Ok(Some(content));
                }
                Err(_) => {}
            }
            
            match self.try_decode_with_rqrr(&gray_up) {
                Ok(content) => {
                    info!("‚úÖ rqrr decoded upscaled ONNX region");
                    return Ok(Some(content));
                }
                Err(_) => {}
            }
        }
        
        // Try 90¬∞ rotations
        for rotation in &[90u32, 180, 270] {
            debug!("üîÑ Trying {}¬∞ rotation", rotation);
            let rotated = match rotation {
                90 => region.rotate90(),
                180 => region.rotate180(),
                270 => region.rotate270(),
                _ => continue,
            };
            let gray_rot = rotated.to_luma8();
            
            if let Ok(content) = self.try_decode_with_rxing(&gray_rot) {
                info!("‚úÖ rxing decoded ONNX region with {}¬∞ rotation", rotation);
                return Ok(Some(content));
            }
            
            if let Ok(content) = self.try_decode_with_rqrr(&gray_rot) {
                info!("‚úÖ rqrr decoded ONNX region with {}¬∞ rotation", rotation);
                return Ok(Some(content));
            }
        }
        
        debug!("‚ùå All decoding attempts failed on ONNX region");
        Ok(None)
    }
    
    /// Try decoding with rxing library
    fn try_decode_with_rxing(&self, gray_image: &image::GrayImage) -> Result<String> {
        let dynamic_image = DynamicImage::ImageLuma8(gray_image.clone());
        let mut multi_detector = rxing::MultiUseMultiFormatReader::default();
        
        let mut bitmap = rxing::BinaryBitmap::new(rxing::common::GlobalHistogramBinarizer::new(
            rxing::BufferedImageLuminanceSource::new(dynamic_image)
        ));
        
        let result = multi_detector.decode_with_state(&mut bitmap)?;
        Ok(result.getText().to_string())
    }
    
    /// Try decoding with rqrr library  
    fn try_decode_with_rqrr(&self, gray_image: &image::GrayImage) -> Result<String> {
        let mut img = rqrr::PreparedImage::prepare(gray_image.clone());
        let grids = img.detect_grids();
        
        if let Some(grid) = grids.first() {
            let (_, content) = grid.decode()?;
            Ok(content)
        } else {
            Err(anyhow::anyhow!("No QR grid detected by rqrr"))
        }
    }

    /// Get model information
    pub fn get_model_info(&self) -> (ModelSize, String) {
        (self.model_size, self.model_path.clone())  
    }
}

impl Default for ModelSize {
    fn default() -> Self {
        ModelSize::Small // Good balance of speed and accuracy
    }
}