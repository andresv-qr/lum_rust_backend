# ğŸ§  AnÃ¡lisis de Memoria: Requests Concurrentes vs RAM Total

## ğŸ¯ Respuesta Directa

**NO, la memoria NO se multiplica por el nÃºmero de requests.**

```
âŒ INCORRECTO:
  1 request  = 100MB
  10 requests = 1000MB (100MB Ã— 10)
  
âœ… CORRECTO:
  1 request  = 100MB
  10 requests = 120-140MB (100MB + overhead pequeÃ±o)
```

**RazÃ³n:** El modelo se carga UNA SOLA VEZ en memoria y se REUTILIZA para todas las requests.

---

## ğŸ”¬ Desglose de Memoria Detallado

### Arquitectura en Memoria

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  MEMORIA TOTAL DEL PROCESO PYTHON                          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                            â”‚
â”‚  ğŸ§  COMPARTIDO (Una sola vez, todas requests comparten):  â”‚
â”‚  â”œâ”€ PyTorch Runtime:           60MB                       â”‚
â”‚  â”œâ”€ Modelo Small (pesos):      22MB                       â”‚
â”‚  â”œâ”€ Modelo Medium (pesos):     52MB  [lazy]              â”‚
â”‚  â”œâ”€ Python overhead:           10MB                       â”‚
â”‚  â””â”€ TOTAL COMPARTIDO:         ~95MB (Small solo)         â”‚
â”‚                                 ~147MB (Small+Medium)     â”‚
â”‚                                                            â”‚
â”‚  ğŸ“¦ PER-REQUEST (se replica por cada request concurrente):â”‚
â”‚  â”œâ”€ Input buffer (imagen):     5-15MB  (depende tamaÃ±o)  â”‚
â”‚  â”œâ”€ Preprocessing buffers:     3-8MB                      â”‚
â”‚  â”œâ”€ Inference tensors:         2-5MB                      â”‚
â”‚  â”œâ”€ Output buffers:            0.5-1MB                    â”‚
â”‚  â””â”€ TOTAL PER-REQUEST:        ~12-30MB                    â”‚
â”‚                                                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“Š Ejemplos Reales: 1 vs 10 vs 100 Requests

### Escenario 1: Small Solo (Recomendado)

```
Base (modelo cargado, sin requests):
â”œâ”€ PyTorch + Small + Python = 95MB
â””â”€ Proceso idle

1 request activa:
â”œâ”€ Base: 95MB
â”œâ”€ Request 1: +15MB (imagen 2MP tÃ­pica)
â””â”€ TOTAL: 110MB

10 requests concurrentes:
â”œâ”€ Base: 95MB
â”œâ”€ Request 1-10: +15MB Ã— 10 = +150MB
â””â”€ TOTAL: 245MB

100 requests concurrentes (extremo):
â”œâ”€ Base: 95MB
â”œâ”€ Requests 1-100: +15MB Ã— 100 = +1500MB
â””â”€ TOTAL: 1595MB (~1.6GB)
```

**ConclusiÃ³n:** Memoria crece LINEALMENTE con concurrencia, pero el modelo (95MB) se comparte.

---

### Escenario 2: Small + Medium Hybrid

```
Base (Small cargado, Medium no):
â”œâ”€ PyTorch + Small + Python = 95MB
â””â”€ Proceso idle

DespuÃ©s de 1ra fallback (Medium cargado):
â”œâ”€ PyTorch + Small + Medium + Python = 147MB
â””â”€ Ambos modelos en memoria

10 requests concurrentes (80% usa Small, 20% Medium):
â”œâ”€ Base compartida: 147MB
â”œâ”€ 8 requests usando Small: 8 Ã— 12MB = 96MB
â”œâ”€ 2 requests usando Medium: 2 Ã— 18MB = 36MB
â””â”€ TOTAL: 279MB

100 requests concurrentes:
â”œâ”€ Base compartida: 147MB
â”œâ”€ 80 usando Small: 80 Ã— 12MB = 960MB
â”œâ”€ 20 usando Medium: 20 Ã— 18MB = 360MB
â””â”€ TOTAL: 1467MB (~1.5GB)
```

---

## ğŸ” Â¿Por QuÃ© No Se Multiplica el Modelo?

### ExplicaciÃ³n TÃ©cnica: Singleton Pattern

El cÃ³digo usa **singleton pattern** - el modelo se carga UNA VEZ y se reutiliza:

```python
# âœ… CORRECTO - Singleton (nuestro cÃ³digo)
qr_reader_small = None  # Variable global

def get_small_reader():
    global qr_reader_small
    if qr_reader_small is None:
        qr_reader_small = QReader(model_size='s')  # Carga UNA VEZ
    return qr_reader_small  # Reutiliza la misma instancia

# Request 1
reader = get_small_reader()  # Carga modelo (95MB)
result = reader.detect(image1)

# Request 2 (concurrente)
reader = get_small_reader()  # REUTILIZA el mismo modelo (0MB adicional)
result = reader.detect(image2)

# Request 3, 4, 5... (todas reusan el mismo modelo)
```

vs

```python
# âŒ INCORRECTO - Sin Singleton (NO HAGAS ESTO)
def detect_qr_bad(image):
    reader = QReader(model_size='s')  # Â¡Carga modelo CADA VEZ!
    return reader.detect(image)

# Request 1
result = detect_qr_bad(image1)  # Carga 95MB

# Request 2
result = detect_qr_bad(image2)  # Â¡Carga OTROS 95MB! = 190MB total âŒ

# 10 requests = 950MB âŒâŒâŒ
```

---

## ğŸ“ˆ Tabla de Consumo Real por Concurrencia

### Small Solo

| Requests Concurrentes | RAM Modelo | RAM Buffers | RAM Total | Notas |
|----------------------|------------|-------------|-----------|-------|
| **0 (idle)** | 95MB | 0MB | 95MB | Modelo cargado, esperando |
| **1** | 95MB | 15MB | 110MB | Request tÃ­pica |
| **5** | 95MB | 75MB | 170MB | 5Ã— buffers |
| **10** | 95MB | 150MB | 245MB | 10Ã— buffers |
| **20** | 95MB | 300MB | 395MB | Inicio de alta carga |
| **50** | 95MB | 750MB | 845MB | Muy alta carga |
| **100** | 95MB | 1500MB | 1595MB | Extremo (poco probable) |

**Nota:** Modelo siempre 95MB, solo buffers crecen.

---

### Small + Medium Hybrid

| Requests Concurrentes | RAM Modelos | RAM Buffers (avg) | RAM Total | Notas |
|----------------------|-------------|-------------------|-----------|-------|
| **0 (idle)** | 95MB | 0MB | 95MB | Solo Small cargado |
| **0 (post-fallback)** | 147MB | 0MB | 147MB | Small + Medium cargados |
| **1** | 147MB | 12MB | 159MB | Request tÃ­pica (Small) |
| **5** | 147MB | 60MB | 207MB | Mix 80/20 Small/Medium |
| **10** | 147MB | 132MB | 279MB | Mix 80/20 |
| **20** | 147MB | 264MB | 411MB | Mix 80/20 |
| **50** | 147MB | 660MB | 807MB | Alta carga |
| **100** | 147MB | 1320MB | 1467MB | Extremo |

**FÃ³rmula:**
```
RAM Total = RAM_Modelos_Compartida + (N_requests Ã— RAM_buffer_promedio)

Donde:
- RAM_Modelos_Compartida = 95MB (Small) o 147MB (Small+Medium)
- N_requests = nÃºmero de requests concurrentes
- RAM_buffer_promedio = 12-15MB por request (imagen tÃ­pica 2MP)
```

---

## ğŸ¯ LÃ­mites PrÃ¡cticos de Concurrencia

### SegÃºn RAM Disponible

**Contenedor con 512MB RAM:**
```
Small Solo:
  Base: 95MB
  Disponible para buffers: 512 - 95 = 417MB
  Requests concurrentes max: 417 / 15 = ~27 requests
  
Small + Medium:
  Base: 147MB
  Disponible para buffers: 512 - 147 = 365MB
  Requests concurrentes max: 365 / 15 = ~24 requests
```

**Contenedor con 1GB RAM:**
```
Small Solo:
  Base: 95MB
  Disponible: 1024 - 95 = 929MB
  Requests max: 929 / 15 = ~61 requests
  
Small + Medium:
  Base: 147MB
  Disponible: 1024 - 147 = 877MB
  Requests max: 877 / 15 = ~58 requests
```

**Contenedor con 2GB RAM:**
```
Small Solo:
  Base: 95MB
  Disponible: 2048 - 95 = 1953MB
  Requests max: 1953 / 15 = ~130 requests
  
Small + Medium:
  Base: 147MB
  Disponible: 2048 - 147 = 1901MB
  Requests max: 1901 / 15 = ~126 requests
```

---

## âš¡ SegÃºn CPU/Latencia

**LÃ­mite mÃ¡s realista es CPU, no RAM:**

```
Un solo CPU core procesa:
â”œâ”€ Small: ~40ms por request
â”œâ”€ Throughput: 1000ms / 40ms = 25 req/s
â””â”€ Concurrencia Ã³ptima: 2-3 requests

4 CPU cores (t4g.medium):
â”œâ”€ Throughput teÃ³rico: 25 Ã— 4 = 100 req/s
â”œâ”€ Concurrencia Ã³ptima: 8-12 requests
â””â”€ Concurrencia max Ãºtil: 20-30 requests

MÃ¡s allÃ¡ de eso, requests esperan en cola (no mejora throughput)
```

**ConclusiÃ³n:** En la prÃ¡ctica, **CPU es el cuello de botella, no RAM.**

---

## ğŸ”§ ConfiguraciÃ³n Recomendada por Volumen

### Bajo Volumen (<1K req/dÃ­a)

```yaml
Container:
  RAM: 512MB
  CPU: 1 vCPU (t4g.small)
  
Config:
  Model: Small solo
  Max concurrent: 5-10
  Queue: Sin lÃ­mite (requests esperan)
  
RAM Usage:
  Idle: 95MB
  Peak (10 concurrent): 245MB
  Safety margin: 267MB (52%)
```

---

### Volumen Medio (1K-10K req/dÃ­a)

```yaml
Container:
  RAM: 1GB
  CPU: 2 vCPU (t4g.medium)
  
Config:
  Model: Small + Medium hybrid
  Max concurrent: 15-20
  Queue timeout: 30s
  
RAM Usage:
  Idle: 147MB
  Peak (20 concurrent): 411MB
  Safety margin: 613MB (60%)
```

---

### Alto Volumen (>10K req/dÃ­a)

```yaml
Option A - Horizontal Scaling (RECOMENDADO):
  Instances: 3Ã— t4g.small
  RAM per instance: 512MB
  Total RAM: 1.5GB
  Total throughput: 75 req/s
  
Option B - Vertical Scaling:
  Instance: 1Ã— t4g.large
  RAM: 2GB
  CPU: 4 vCPU
  Throughput: 100 req/s
  
RecomendaciÃ³n: Option A (mÃ¡s resiliente)
```

---

## ğŸš¨ Anti-Patterns a Evitar

### âŒ Error 1: Crear Instancia del Modelo por Request

```python
# âŒ MAL - Carga modelo cada vez
@app.route('/detect', methods=['POST'])
def detect():
    reader = QReader(model_size='s')  # 95MB CADA VEZ âŒ
    return reader.detect(image)

# 10 requests concurrentes = 950MB RAM âŒâŒâŒ
```

### âœ… Correcto: Singleton

```python
# âœ… BIEN - Carga UNA VEZ
qr_reader = None

def get_reader():
    global qr_reader
    if qr_reader is None:
        qr_reader = QReader(model_size='s')  # 95MB UNA VEZ
    return qr_reader

@app.route('/detect', methods=['POST'])
def detect():
    reader = get_reader()  # Reutiliza instancia
    return reader.detect(image)

# 10 requests concurrentes = 245MB RAM âœ…
```

---

### âŒ Error 2: Flask/Gunicorn con MÃºltiples Workers

```python
# âŒ MAL - Cada worker carga su propio modelo
# gunicorn --workers 4 app:app

Worker 1: 95MB modelo + 50MB buffers = 145MB
Worker 2: 95MB modelo + 50MB buffers = 145MB
Worker 3: 95MB modelo + 50MB buffers = 145MB
Worker 4: 95MB modelo + 50MB buffers = 145MB
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
TOTAL:    380MB modelos + 200MB buffers = 580MB âŒ

Â¡El modelo se carga 4 VECES! âŒâŒâŒ
```

### âœ… Correcto: HTTPServer Simple (nuestro script)

```python
# âœ… BIEN - Un solo proceso, modelo compartido
# python qr_fallback_small_medium.py

Proceso Ãºnico:
â”œâ”€ 95MB modelo (una sola copia)
â”œâ”€ Threading maneja concurrencia
â””â”€ TOTAL: 95MB + (N Ã— 15MB buffers)

10 requests: 95 + 150 = 245MB âœ…
```

---

## ğŸ“Š ComparaciÃ³n: Singleton vs Multi-Worker

### Caso: 20 Requests Concurrentes

**OpciÃ³n A: HTTPServer con Singleton (Nuestro Script)**
```
1 proceso:
â”œâ”€ Modelos compartidos: 147MB (Small + Medium)
â”œâ”€ 20 buffers: 20 Ã— 15MB = 300MB
â””â”€ TOTAL: 447MB âœ…âœ…âœ…

Throughput: ~50 req/s (con 4 CPU cores)
```

**OpciÃ³n B: Gunicorn 4 Workers (Anti-pattern)**
```
4 procesos:
â”œâ”€ Worker 1: 147MB modelo + 75MB buffers = 222MB
â”œâ”€ Worker 2: 147MB modelo + 75MB buffers = 222MB
â”œâ”€ Worker 3: 147MB modelo + 75MB buffers = 222MB
â”œâ”€ Worker 4: 147MB modelo + 75MB buffers = 222MB
â””â”€ TOTAL: 888MB âŒâŒâŒ

Throughput: ~50 req/s (mismo que Option A)

Desperdicio: 888 - 447 = 441MB (98% mÃ¡s RAM)
```

**ConclusiÃ³n:** Gunicorn NO tiene beneficio aquÃ­, solo desperdicia RAM.

---

## ğŸ¯ Resumen Ejecutivo

### Para tu Sistema de Facturas

**ConfiguraciÃ³n Recomendada:**
```yaml
Deployment:
  Container: t4g.small (1GB RAM, 2 vCPU)
  Script: qr_fallback_small_medium.py (singleton)
  
Modelos:
  Small: Siempre cargado (95MB)
  Medium: Lazy loading (52MB adicional)
  
RAM Breakdown:
  Base (Small solo):           95MB
  Base (Small + Medium):       147MB
  Por request concurrente:     +12-15MB
  
LÃ­mites Seguros:
  1GB RAM container:
    - Max concurrente: 50-60 requests
    - Recomendado: 20-30 requests (safety margin)
    - Throughput: 40-50 req/s
  
Volumen Esperado (facturas):
  TÃ­pico: 5-10 req/s (bajo)
  Pico: 20-30 req/s (manejable)
  â†’ 1GB RAM es MÃS QUE SUFICIENTE âœ…
```

### CÃ¡lculo Simple

```
Para N requests concurrentes:

RAM Total = Base + (N Ã— Buffer)

Donde:
- Base = 95MB (Small) o 147MB (Small+Medium)
- N = requests concurrentes
- Buffer = 15MB promedio

Ejemplos:
â”œâ”€ 10 concurrent: 147 + (10 Ã— 15) = 297MB
â”œâ”€ 20 concurrent: 147 + (20 Ã— 15) = 447MB
â”œâ”€ 50 concurrent: 147 + (50 Ã— 15) = 897MB
â””â”€ 100 concurrent: 147 + (100 Ã— 15) = 1647MB
```

### Tu Caso Real (Estimado)

```
Facturas por dÃ­a: 1000-5000
Requests por segundo promedio: 0.5-3 req/s
Requests por segundo pico: 10-15 req/s (batch upload)

Concurrencia realista:
â”œâ”€ Promedio: 2-3 requests simultÃ¡neas
â”œâ”€ Pico: 8-12 requests simultÃ¡neas
â””â”€ RAM necesaria: 147 + (12 Ã— 15) = 327MB

Container 1GB RAM â†’ Suficiente con margen 300% âœ…âœ…âœ…
```

---

## ğŸš€ ConclusiÃ³n

**Respuesta a tu pregunta:**

âŒ **NO**, 10 requests NO significa 1000MB (100MB Ã— 10)

âœ… **SÃ**, 10 requests significa ~250-280MB:
- Modelo compartido: 147MB (una sola vez)
- Buffers: 10 Ã— 13MB = 130MB
- **Total: 277MB**

El modelo se carga **UNA SOLA VEZ** y se **REUTILIZA** para todas las requests.

Solo los **buffers temporales** (imagen, tensors intermedios) se multiplican por concurrencia.

**Para tu caso:**
- Container 1GB RAM es mÃ¡s que suficiente
- Puedes manejar 50+ requests concurrentes sin problema
- En la prÃ¡ctica, CPU serÃ¡ el lÃ­mite (20-30 req/s), no RAM

**Nuestro script usa singleton correctamente, asÃ­ que estÃ¡s protegido** âœ…

