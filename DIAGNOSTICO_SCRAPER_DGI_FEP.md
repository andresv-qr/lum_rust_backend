# üîç DIAGN√ìSTICO: Problema con Extracci√≥n de Productos en DGI-FEP

**Fecha**: Octubre 22, 2025  
**URL Problema**: https://dgi-fep.mef.gob.pa/Consultas/FacturasPorQR  
**S√≠ntoma**: La secci√≥n de detalle de productos NO se est√° extrayendo correctamente

---

## üìä AN√ÅLISIS DEL C√ìDIGO ACTUAL

### **Estado del Scraper**:

1. **`scraper_service_clean.rs`** (L140-146):
   ```rust
   async fn perform_scraping(...) -> Result<...> {
       // TODO: Temporarily disabled to allow compilation for other tests.
       Err(InvoiceProcessingError::ScrapingError {
           message: "Scraping logic is temporarily disabled.".to_string(),
           ...
       })
   }
   ```
   ‚ö†Ô∏è **PROBLEMA PRINCIPAL**: El scraper est√° **DESHABILITADO COMPLETAMENTE**
   
2. **`ocr_extractor_xpath_v2.rs`** (L205-245):
   - Funci√≥n `extract_line_items()` implementada correctamente ‚úÖ
   - Usa XPath para buscar productos: `//td[@data-title='Cantidad']`
   - Extrae campos: Cantidad, C√≥digo, Descripci√≥n, Descuento, Precio, Impuesto

---

## üßê POSIBLES CAUSAS DEL PROBLEMA

### **1. Scraper Deshabilitado (CONFIRMADO)**
```rust
// En scraper_service_clean.rs l√≠nea 133
async fn perform_scraping(...) {
    // TODO: Temporarily disabled
    Err(...)  // ‚ùå Siempre retorna error
}
```

**Soluci√≥n**: Reconectar la l√≥gica de extracci√≥n.

---

### **2. XPath Selector Incorrecto o Estructura HTML Cambi√≥**

**XPath actual** (L216):
```rust
"//td[@data-title='Cantidad']"
```

**Problema potencial**:
- La p√°gina de DGI-FEP puede haber cambiado su estructura HTML
- Los atributos `data-title` pueden ser diferentes
- Los productos pueden estar en un `<table>` con diferente clase/id

**Necesito verificar**: ¬øLa p√°gina usa `data-title="Cantidad"` en los `<td>`?

---

### **3. Contenido Din√°mico (JavaScript)**

La p√°gina DGI-FEP probablemente carga productos v√≠a JavaScript:

```html
<!-- HTML inicial (lo que ve el scraper): -->
<div id="productos-container">
    <div class="loading">Cargando productos...</div>
</div>

<!-- HTML despu√©s de JavaScript (lo que ve el usuario): -->
<table class="productos">
    <tr>
        <td data-title="Cantidad">2</td>
        <td data-title="Descripci√≥n">Producto X</td>
        ...
    </tr>
</table>
```

**El scraper Rust usa `reqwest`**, que solo obtiene HTML est√°tico (no ejecuta JavaScript).

**Soluci√≥n potencial**: Usar un navegador headless como:
- Chromiumoxide (Rust)
- Headless Chrome via API
- Selenium/Puppeteer (Python/Node)

---

### **4. Estructura de Tabla Anidada**

El HTML real puede ser m√°s complejo:

```html
<div class="factura-detalle">
    <table class="resumen">...</table>  <!-- Primera tabla -->
    
    <table class="productos">           <!-- Segunda tabla con productos -->
        <tbody>
            <tr>
                <td data-title="Cantidad">2</td>
                <td data-title="Descripci√≥n">Pan</td>
                ...
            </tr>
        </tbody>
    </table>
</div>
```

**XPath actual** busca CUALQUIER `<td data-title="Cantidad">` en TODA la p√°gina.

**Puede estar capturando**:
- Totales en vez de productos
- Elementos de resumen
- Tablas incorrectas

**XPath m√°s espec√≠fico**:
```rust
// Opci√≥n A: Solo dentro de tbody
"//tbody/tr/td[@data-title='Cantidad']"

// Opci√≥n B: Solo en tabla con clase espec√≠fica
"//table[contains(@class, 'productos')]//td[@data-title='Cantidad']"

// Opci√≥n C: Excluir tabla de resumen
"//table[not(contains(@class, 'resumen'))]//td[@data-title='Cantidad']"
```

---

## üîß SOLUCIONES PROPUESTAS

### **SOLUCI√ìN 1: Reconectar el Scraper (URGENTE)**

**Archivo**: `src/api/invoice_processor/scraper_service_clean.rs`

```rust
// CAMBIAR ESTO (l√≠nea 133-146):
async fn perform_scraping(...) -> Result<...> {
    Err(InvoiceProcessingError::ScrapingError {
        message: "Scraping logic is temporarily disabled.".to_string(),
        ...
    })
}

// POR ESTO:
async fn perform_scraping(
    &self,
    url: &str,
    user_id: &str,
    user_email: &str,
    origin: &str,
    invoice_type: &str,
    reception_date: DateTime<Utc>,
    process_date: DateTime<Utc>,
) -> Result<(FullInvoiceData, u32), InvoiceProcessingError> {
    use crate::processing::web_scraping::ocr_extractor_xpath_v2;
    use crate::processing::web_scraping::http_client;
    
    // 1. Fetch HTML
    let client = reqwest::Client::new();
    let html_content = http_client::fetch_invoice_html(&client, url)
        .await
        .map_err(|e| InvoiceProcessingError::ScrapingError {
            message: format!("Failed to fetch HTML: {}", e),
            error_type: ErrorType::NetworkError,
            retry_attempts: 0,
        })?;
    
    // 2. Extract main info
    let main_info = ocr_extractor_xpath_v2::extract_main_info(&html_content)
        .map_err(|e| InvoiceProcessingError::ScrapingError {
            message: format!("Failed to extract main info: {}", e),
            error_type: ErrorType::ExtractionError,
            retry_attempts: 0,
        })?;
    
    // 3. Extract line items (PRODUCTOS)
    let line_items = ocr_extractor_xpath_v2::extract_line_items(&html_content)
        .map_err(|e| InvoiceProcessingError::ScrapingError {
            message: format!("Failed to extract line items: {}", e),
            error_type: ErrorType::ExtractionError,
            retry_attempts: 0,
        })?;
    
    // 4. Extract payment data
    let payment_data = ocr_extractor_xpath_v2::extract_payment_data(&html_content)
        .unwrap_or_default();
    
    // 5. Build FullInvoiceData
    let invoice_data = FullInvoiceData {
        user_id: user_id.to_string(),
        user_email: user_email.to_string(),
        origin: origin.to_string(),
        invoice_type: invoice_type.to_string(),
        reception_date,
        process_date,
        invoice_url: Some(url.to_string()),
        main_info,
        line_items,
        payment_data,
        // ... otros campos
    };
    
    let fields_count = invoice_data.line_items.len() as u32;
    
    Ok((invoice_data, fields_count))
}
```

---

### **SOLUCI√ìN 2: Mejorar el XPath de Productos**

**Archivo**: `src/processing/web_scraping/ocr_extractor_xpath_v2.rs`

Cambiar l√≠nea 216:

```rust
// ACTUAL (puede capturar elementos incorrectos):
if let Ok(xpath) = factory.build("//td[@data-title='Cantidad']") {

// MEJORADO (m√°s espec√≠fico):
if let Ok(xpath) = factory.build("//tbody/tr/td[@data-title='Cantidad']") {
    // O incluso mejor:
    // "//table[contains(@class, 'detalle')]//tbody/tr/td[@data-title='Cantidad']"
```

**Agregar logging para debugging**:

```rust
pub fn extract_line_items(html_content: &str) -> Result<Vec<HashMap<String, String>>> {
    println!("üîç HTML length: {} bytes", html_content.len());
    
    let package = parser::parse(html_content)
        .map_err(|e| anyhow::anyhow!("Error parsing HTML: {}", e))?;
    let document = package.as_document();
    
    let factory = Factory::new();
    let context = Context::new();
    
    let mut items = Vec::new();
    
    // Debug: Probar diferentes XPath
    let xpaths_to_try = vec![
        "//td[@data-title='Cantidad']",
        "//tbody/tr/td[@data-title='Cantidad']",
        "//table[@class='table-detalle']//td[@data-title='Cantidad']",
        "//div[@id='detalle']//td[@data-title='Cantidad']",
    ];
    
    for xpath_expr in xpaths_to_try {
        println!("üîç Trying XPath: {}", xpath_expr);
        
        if let Ok(xpath) = factory.build(xpath_expr) {
            if let Ok(result) = xpath.evaluate(&context, document.root()) {
                if let Value::Nodeset(quantity_nodes) = result {
                    println!("‚úÖ Found {} nodes with XPath: {}", quantity_nodes.size(), xpath_expr);
                    
                    if quantity_nodes.size() > 0 {
                        // Procesar con este XPath que funciona
                        for quantity_node in quantity_nodes {
                            // ... extraer item
                        }
                        break;
                    }
                }
            }
        }
    }
    
    // ... resto del c√≥digo
}
```

---

### **SOLUCI√ìN 3: Implementar Navegador Headless (Si JavaScript es necesario)**

Si la p√°gina requiere JavaScript, necesitas usar un navegador:

**Opci√≥n A: Chromiumoxide (Rust nativo)**

```rust
// Agregar a Cargo.toml:
chromiumoxide = "0.5"

// En scraper_service_clean.rs:
use chromiumoxide::browser::{Browser, BrowserConfig};

async fn fetch_with_browser(url: &str) -> Result<String> {
    let (browser, mut handler) = Browser::launch(
        BrowserConfig::builder()
            .no_sandbox()
            .build()
            .map_err(|e| anyhow!("Failed to start browser: {}", e))?
    ).await?;
    
    let handle = tokio::spawn(async move {
        loop {
            let _ = handler.next().await;
        }
    });
    
    let page = browser.new_page("about:blank").await?;
    page.goto(url).await?;
    page.wait_for_navigation().await?;
    
    // Esperar a que carguen los productos
    tokio::time::sleep(std::time::Duration::from_secs(2)).await;
    
    let html = page.content().await?;
    
    browser.close().await?;
    handle.abort();
    
    Ok(html)
}
```

**Opci√≥n B: Llamar a Python con Selenium** (m√°s f√°cil de debuggear):

```python
# scraper_dgi_fep.py
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
import sys

url = sys.argv[1]

options = webdriver.ChromeOptions()
options.add_argument('--headless')
driver = webdriver.Chrome(options=options)

driver.get(url)

# Esperar a que cargue la tabla de productos
WebDriverWait(driver, 10).until(
    EC.presence_of_element_located((By.XPATH, "//td[@data-title='Cantidad']"))
)

html = driver.page_source
print(html)

driver.quit()
```

```rust
// Llamar desde Rust:
use tokio::process::Command;

async fn fetch_with_python_selenium(url: &str) -> Result<String> {
    let output = Command::new("python3")
        .arg("scraper_dgi_fep.py")
        .arg(url)
        .output()
        .await?;
    
    let html = String::from_utf8(output.stdout)?;
    Ok(html)
}
```

---

### **SOLUCI√ìN 4: Guardar HTML para An√°lisis**

**Debug script** para ver qu√© HTML se est√° obteniendo:

```rust
// En ocr_extractor_xpath_v2.rs
pub fn extract_line_items(html_content: &str) -> Result<Vec<HashMap<String, String>>> {
    // GUARDAR HTML PARA DEBUGGING
    use std::fs;
    let debug_path = format!("/tmp/dgi_fep_debug_{}.html", chrono::Utc::now().timestamp());
    fs::write(&debug_path, html_content).ok();
    println!("üìÑ HTML saved to: {}", debug_path);
    
    // ... resto del c√≥digo
}
```

Luego puedes:
1. Ejecutar el scraper
2. Abrir `/tmp/dgi_fep_debug_XXXX.html` en un editor
3. Buscar manualmente c√≥mo est√°n estructurados los productos
4. Ajustar el XPath basado en la estructura real

---

## üéØ PLAN DE ACCI√ìN RECOMENDADO

### **Paso 1: Reconectar el Scraper** (5 min)
- Implementar SOLUCI√ìN 1
- Reconectar `perform_scraping()` con la l√≥gica de extracci√≥n

### **Paso 2: Agregar Debug Logging** (3 min)
- Agregar `println!` para ver qu√© se est√° extrayendo
- Guardar HTML en archivo temporal

### **Paso 3: Probar con URL Real** (2 min)
```bash
# Test directo
curl -X POST http://localhost:8000/api/v4/process_from_url \
  -H "Content-Type: application/json" \
  -d '{
    "url": "https://dgi-fep.mef.gob.pa/Consultas/FacturasPorQR?chFE=...",
    "user_id": "test",
    "user_email": "test@test.com"
  }'
```

### **Paso 4: Analizar HTML Real** (10 min)
- Abrir `/tmp/dgi_fep_debug_XXXX.html`
- Buscar estructura de productos
- Ajustar XPath si es necesario

### **Paso 5: Si no funciona, implementar navegador headless** (30 min)
- Usar Chromiumoxide o Python+Selenium
- Ejecutar JavaScript antes de extraer

---

## üìä COMPARACI√ìN DE SOLUCIONES

| Soluci√≥n | Dificultad | Tiempo | Efectividad | Recomendado |
|----------|-----------|--------|-------------|-------------|
| 1. Reconectar scraper | ‚≠ê F√°cil | 5 min | Alta (si HTML es est√°tico) | ‚úÖ **PRIMERO** |
| 2. Mejorar XPath | ‚≠ê‚≠ê Media | 10 min | Alta (si selector est√° mal) | ‚úÖ **SEGUNDO** |
| 3. Debug logging | ‚≠ê F√°cil | 3 min | Alta (para diagnosticar) | ‚úÖ **SIEMPRE** |
| 4. Navegador headless | ‚≠ê‚≠ê‚≠ê Dif√≠cil | 30-60 min | Muy Alta (JavaScript) | ‚ö†Ô∏è **Si nada funciona** |

---

## ‚ùì PREGUNTAS PARA TI

Para ayudarte mejor, necesito saber:

1. **¬øEl scraper est√° corriendo actualmente?**
   ```bash
   curl -X POST http://localhost:8000/api/v4/process_from_url \
     -H "Content-Type: application/json" \
     -d '{"url": "https://dgi-fep.mef.gob.pa/...", ...}'
   ```
   ¬øQu√© error retorna?

2. **¬øLa p√°gina DGI-FEP usa JavaScript para cargar productos?**
   - Abre la URL en un navegador
   - Deshabilita JavaScript (DevTools ‚Üí Settings ‚Üí Debugger ‚Üí Disable JavaScript)
   - ¬øSigues viendo los productos? 
     - **S√≠** ‚Üí HTML est√°tico, SOLUCI√ìN 1-2 funcionar√°
     - **No** ‚Üí Requiere JavaScript, necesitas SOLUCI√ìN 4

3. **¬øQu√© versi√≥n del extractor est√°s usando?**
   - `ocr_extractor.rs` (CSS selectors)
   - `ocr_extractor_xpath.rs` (XPath v1)
   - `ocr_extractor_xpath_v2.rs` (XPath v2) ‚Üê **Recomendado**

---

## üöÄ ¬øQUIERES QUE IMPLEMENTE LA SOLUCI√ìN?

Dime:
- ‚úÖ **S√≠, reconecta el scraper** (SOLUCI√ìN 1)
- ‚úÖ **S√≠, agrega debug logging** (SOLUCI√ìN 3)
- ‚úÖ **S√≠, mejora el XPath** (SOLUCI√ìN 2)
- ‚è≥ **Necesito navegador headless** (SOLUCI√ìN 4)

O si prefieres, puedo:
1. Implementar todo de una vez
2. Solo reconectar el scraper y ver qu√© pasa
3. Crear un script de testing para diagnosticar

¬øQu√© prefieres? ü§ì
